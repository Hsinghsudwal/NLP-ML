{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "739c40c2-b3c6-41fe-a53a-6dba1b029520",
   "metadata": {},
   "source": [
    "# 1. Tokenization and Text Cleaning\n",
    "\n",
    "At the heart of NLP lies the art of breaking down text into meaningful units. Tokenization is the process of splitting text into words, phrases, or even sentences (tokens). It's the initial step that sets the stage for further analysis. Coupled with text cleaning, where we remove unnecessary characters, numbers, and symbols, tokenization ensures we work with pristine, understandable language units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c38a5648-9ff8-4ee7-818f-9a58400c9704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nlp', 'is', 'amazing', 'let', 'explore', 'its', 'wonders']\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "# Example Tokenization and Text Cleaning\n",
    "import nltk \n",
    "text = \"NLP is amazing! Let's explore its wonders.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "cleaned_tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd61ab3e-6eb6-44e4-8eae-aff768358e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loving the new ! Best phone ever! '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)  # Remove mentions\n",
    "    tweet = re.sub(r'#\\w+', '', tweet)  # Remove hashtags\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)  # Remove URLs\n",
    "    return tweet\n",
    "\n",
    "tweet = \"Loving the new #iPhone! Best phone ever! @Apple\"\n",
    "clean_tweet(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dad801-af87-4c86-81f2-cbc45c7b8a99",
   "metadata": {},
   "source": [
    "# 2. Stop Words Removing\n",
    "\n",
    "Not all words contribute equally to the meaning of a sentence. Stop words like \"the\" or \"and\" are often filtered out to focus on more meaningful content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0daa6229-d527-4038-a6ce-1884572d0c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nlp', 'amazing', 'let', 'explore', 'wonders']\n"
     ]
    }
   ],
   "source": [
    "# Example Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_sentence = [word for word in cleaned_tokens if word not in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1b9645-802d-453b-9d4e-e4c213c0960e",
   "metadata": {},
   "source": [
    "# 3. Stemming & Lemmatizing\n",
    "\n",
    "Stemming and lemmatization are both text normalization techniques used in Natural Language Processing (NLP) to reduce words to their base or root forms. While they share the goal of simplifying words, they operate differently in terms of the linguistic knowledge they apply.\n",
    "\n",
    "Stemming: Stemming involves cutting off prefixes or suffixes of words to obtain their root or base form, known as the stem. The purpose is to treat words with similar meanings as if they were the same. Stemming is a rule-based method that doesn't always result in a valid word, but it's computationally less intensive.\n",
    "\n",
    "Lemmatization: Lemmatization, on the other hand, involves reducing words to their base or dictionary forms, known as lemmas. It takes into account the context of the word in a sentence and applies morphological analysis. Lemmatization results invalid words and is more linguistically informed compared to stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe567a34-53b9-46f4-8cb0-217d74da62af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nlp', 'amaz', 'let', 'explor', 'wonder']\n",
      "['nlp', 'amazing', 'let', 'explore', 'wonder']\n"
     ]
    }
   ],
   "source": [
    "# Example Stemming, and Lemmatization \n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_sentence]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_sentence]\n",
    "\n",
    "print(stemmed_words)\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8387b636-ffd3-49d6-872e-e5331ee1ce3b",
   "metadata": {},
   "source": [
    "# 4. Part-of-Speech Tagging:\n",
    "\n",
    "Part-of-speech tagging (POS tagging) is a natural language processing task where the goal is to assign a grammatical category (such as noun, verb, adjective, etc.) to each word in a given text. This provides a deeper understanding of the structure and function of each word in a sentence.\n",
    "The Penn Treebank POS Tag Set is a widely used standard for representing these part-of-speech tags in English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78975a19-f5d3-445e-b2a6-5dfd30a328bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nlp', 'RB'), ('amazing', 'JJ'), ('let', 'NN'), ('explore', 'NN'), ('wonders', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "# Example Part-of-Speech Tagging \n",
    "from nltk import pos_tag\n",
    "\n",
    "pos_tags = nltk.pos_tag(filtered_sentence)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc5eb5-95fe-480b-bade-bd24992728ca",
   "metadata": {},
   "source": [
    "# 5. Named Entity Recognition (NER):\n",
    "\n",
    "NER takes language understanding to the next level by identifying and classifying entities like names, locations, organizations, etc., in a given text. This is crucial for extracting meaningful information from unstructured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2e02c15-0cf7-42a4-80dd-3cb2cf66c23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S nlp/RB amazing/JJ let/NN explore/NN wonders/NNS)\n"
     ]
    }
   ],
   "source": [
    "# Example Named Entity Recognition (NER) \n",
    "from nltk import ne_chunk\n",
    "\n",
    "ner_tags = ne_chunk(pos_tags)\n",
    "print(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e5d5b-1dca-4dbb-ada4-d83b0b2206ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
