{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c152066-820d-4f6b-93c4-a9ccddab02d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sudwa\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\sudwa\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\sudwa\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sudwa\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sudwa\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sudwa\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cfa5267-f50b-4118-8aeb-bf8e97cd2fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Here, content is extracted from the original data, but the extracted content is not modified in any way. Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above. For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[10] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "summary of original text :\n",
      " Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above. For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.\n"
     ]
    }
   ],
   "source": [
    "# libraries \n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "   \n",
    "# input text - to summarize  \n",
    "text = input('')\n",
    "   \n",
    "# tokenizing the text \n",
    "stopWords = set(stopwords.words(\"english\")) \n",
    "words = word_tokenize(text) \n",
    "   \n",
    "# creating a frequency table to keep score for each word \n",
    "   \n",
    "freqTable = dict() \n",
    "for word in words: \n",
    "    word = word.lower() \n",
    "    if word in stopWords: \n",
    "        continue\n",
    "    if word in freqTable: \n",
    "        freqTable[word] += 1\n",
    "    else: \n",
    "        freqTable[word] = 1\n",
    "   \n",
    "\n",
    "# creating a dictionary to keep the score of each sentence \n",
    "sentences = sent_tokenize(text) \n",
    "sentenceValue = dict() \n",
    "   \n",
    "for sentence in sentences: \n",
    "    for word, freq in freqTable.items(): \n",
    "        if word in sentence.lower(): \n",
    "            if sentence in sentenceValue: \n",
    "                sentenceValue[sentence] += freq \n",
    "            else: \n",
    "                sentenceValue[sentence] = freq \n",
    "   \n",
    "   \n",
    "   \n",
    "sumValues = 0\n",
    "for sentence in sentenceValue: \n",
    "    sumValues += sentenceValue[sentence] \n",
    "   \n",
    "# Average value of a sentence from the original text \n",
    "   \n",
    "average = int(sumValues / len(sentenceValue)) \n",
    "   \n",
    "# sentences into our summary. \n",
    "summary = '' \n",
    "for sentence in sentences: \n",
    "    if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)):\n",
    "        summary += \" \" + sentence\n",
    "\n",
    "print(\"\\nsummary of original text :\")\n",
    "print(summary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f0557-256e-432c-9a13-50237b0cc1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
